{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18e48829-c766-45a1-acb0-3252bc4f7c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils import *\n",
    "import jax.numpy as np\n",
    "\n",
    "from optimize import *\n",
    "from regression import LogisticRegression\n",
    "\n",
    "import math\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192399f9-cb8a-4226-b7d2-053852555a85",
   "metadata": {},
   "source": [
    "# Plotting code\n",
    "\n",
    "This will show a scatterplot. Each array is a list of optimization runs, containing the training and validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0563205-f4d1-4188-8093-e52faf71c4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatterplot(saga_losses, sgd_losses):\n",
    "    plt.scatter(saga_losses[:,0], saga_losses[:,1], label = 'SAGA')\n",
    "    plt.scatter(sgd_losses[:,0], sgd_losses[:,1], label = 'SGD') \n",
    "    \n",
    "    plt.xlabel('Training loss')\n",
    "    plt.ylabel('Validation loss')\n",
    "    plt.title('A comparison of SAGA and SGD in convergence')\n",
    "    plt.legend()\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3180466-ba89-4e72-b9c2-35a44fa0fa92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_data(X_train, Y_train, X_val, Y_val, label_0, label_1):\n",
    "    train_query = np.logical_or(Y_train==label_0, Y_train==label_1)\n",
    "    val_query = np.logical_or(Y_val==label_0, Y_val==label_1)\n",
    "\n",
    "    Y_train_query = Y_train[train_query]\n",
    "    Y_train_mapped = np.where(Y_train_query == label_0, np.zeros(Y_train_query.shape[0]), np.ones(Y_train_query.shape[0]))\n",
    "\n",
    "    Y_val_query = Y_val[val_query]\n",
    "    Y_val_mapped = np.where(Y_val_query == label_0, np.zeros(Y_val_query.shape[0]), np.ones(Y_val_query.shape[0]))\n",
    "\n",
    "    return X_train[train_query][:2500], Y_train_mapped[:2500], X_val[val_query], Y_val_mapped\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "945c79d5-09c2-492f-a818-09bc92099eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'cifar10'\n",
    "X_train_all = np.load('X_'+dataset+'_train.npy')\n",
    "Y_train_all = np.load('Y_'+dataset+'_train.npy')\n",
    "X_val_all = np.load('X_'+dataset+'_val.npy')\n",
    "Y_val_all = np.load('Y_'+dataset+'_val.npy')\n",
    "\n",
    "cifar10_labels = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "label_to_ind = dict([(cifar10_labels[i], i) for i in range(len(cifar10_labels))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f280f4b4-5862-4330-a5ff-51eaff549807",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train, X_val, Y_val = gather_data(X_train_all, Y_train_all, X_val_all, Y_val_all, label_to_ind['horse'], label_to_ind['dog'])\n",
    "\n",
    "N = X_train.shape[0]\n",
    "D = X_train.shape[1]\n",
    "\n",
    "# number of optimization steps\n",
    "T = 2**14\n",
    "\n",
    "# fix the seed\n",
    "seed = 0\n",
    "\n",
    "n_runs = 50\n",
    "\n",
    "# run optimization below, with step sizes set to (L = Lipschitz constant):\n",
    "# * SGD (diminishing): starting step size is 2/L, ending step size is 1e-2/L\n",
    "# * SAGA (constant): step size is .1/L\n",
    "\n",
    "# no need to log anything during optimization -> use in place of the loss function\n",
    "no_logger = lambda x : 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a0b978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: add validation accuracy for all notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c6f3a0d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [51:29<00:00, 64.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.67600393 0.6823539 ]\n",
      " [0.6763589  0.6828391 ]\n",
      " [0.67478544 0.6808561 ]\n",
      " [0.6763863  0.6828727 ]\n",
      " [0.67531455 0.68146855]\n",
      " [0.6753855  0.6812219 ]\n",
      " [0.67600423 0.68234915]\n",
      " [0.6761451  0.6823034 ]\n",
      " [0.6764798  0.68272793]\n",
      " [0.6756059  0.6817825 ]\n",
      " [0.67520875 0.681042  ]\n",
      " [0.6759997  0.68194586]\n",
      " [0.6761306  0.6825031 ]\n",
      " [0.67653966 0.6830091 ]\n",
      " [0.6761168  0.6823633 ]\n",
      " [0.67536175 0.6816816 ]\n",
      " [0.6760668  0.6822971 ]\n",
      " [0.67580384 0.68239605]\n",
      " [0.67590487 0.682379  ]\n",
      " [0.675095   0.6810483 ]\n",
      " [0.6759055  0.68235123]\n",
      " [0.67557    0.6817206 ]\n",
      " [0.67513627 0.6809786 ]\n",
      " [0.6746265  0.680948  ]\n",
      " [0.6761301  0.68200237]\n",
      " [0.6746451  0.68061733]\n",
      " [0.67714566 0.6835814 ]\n",
      " [0.6751761  0.6814831 ]\n",
      " [0.67606205 0.68238956]\n",
      " [0.6760505  0.68206966]\n",
      " [0.6760649  0.68228716]\n",
      " [0.6758051  0.68165493]\n",
      " [0.67583394 0.68213856]\n",
      " [0.67611426 0.6824621 ]\n",
      " [0.67461485 0.6806752 ]\n",
      " [0.6756427  0.6821913 ]\n",
      " [0.67603785 0.6823786 ]\n",
      " [0.6742057  0.67977935]\n",
      " [0.6760973  0.68265665]\n",
      " [0.67532986 0.6818648 ]\n",
      " [0.67646617 0.68263495]\n",
      " [0.67625123 0.6824845 ]\n",
      " [0.6766993  0.68279576]\n",
      " [0.6751403  0.68143314]\n",
      " [0.67438173 0.68037003]\n",
      " [0.67627686 0.68252456]\n",
      " [0.6767508  0.6832818 ]\n",
      " [0.6762805  0.68259925]\n",
      " [0.6748096  0.6809376 ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# collect all SGD datapoints first\n",
    "sgd_pairs = []\n",
    "\n",
    "# only calc lipshitz constant once\n",
    "# change random seed\n",
    "key = init_prng(0)\n",
    "lr = LogisticRegression(X_train, Y_train, X_val, Y_val, beta = 0)\n",
    "w0 = lr.initialization()\n",
    "data_sampler = minibatch_sampler(N, 1)\n",
    "descent_func = lambda w, data_samples: lr.train_loss_and_grad(w, data_samples)[1]\n",
    "# TODO: only calc this once\n",
    "L = lr.lipschitz_constant() \n",
    "step_size = diminishing_step_size(2/L, 1e-2/L, T)\n",
    "update_method = gd()\n",
    "wF, losses = optimize(w0, data_sampler, no_logger, descent_func, step_size, update_method, T, verbose=False)\n",
    "sgd_pairs.append([lr.train_loss(wF), lr.validation_loss(wF)])\n",
    "\n",
    "for i in tqdm(range(1,n_runs-1)):\n",
    "    \n",
    "    # change random seed\n",
    "    key = init_prng(i)\n",
    "    lr = LogisticRegression(X_train, Y_train, X_val, Y_val, beta = 0)\n",
    "    w0 = lr.initialization()\n",
    "    wF, losses = optimize(w0, data_sampler, no_logger, descent_func, step_size, update_method, T, verbose=False)\n",
    "\n",
    "    sgd_pairs.append([lr.train_loss(wF), lr.validation_loss(wF)])\n",
    "\n",
    "sgd_pairs = np.array(sgd_pairs)\n",
    "print(sgd_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e7a68d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.67600393 0.6823539 ]\n",
      " [0.6763589  0.6828391 ]\n",
      " [0.67478544 0.6808561 ]\n",
      " [0.6763863  0.6828727 ]\n",
      " [0.67531455 0.68146855]\n",
      " [0.6753855  0.6812219 ]\n",
      " [0.67600423 0.68234915]\n",
      " [0.6761451  0.6823034 ]\n",
      " [0.6764798  0.68272793]\n",
      " [0.6756059  0.6817825 ]\n",
      " [0.67520875 0.681042  ]\n",
      " [0.6759997  0.68194586]\n",
      " [0.6761306  0.6825031 ]\n",
      " [0.67653966 0.6830091 ]\n",
      " [0.6761168  0.6823633 ]\n",
      " [0.67536175 0.6816816 ]\n",
      " [0.6760668  0.6822971 ]\n",
      " [0.67580384 0.68239605]\n",
      " [0.67590487 0.682379  ]\n",
      " [0.675095   0.6810483 ]\n",
      " [0.6759055  0.68235123]\n",
      " [0.67557    0.6817206 ]\n",
      " [0.67513627 0.6809786 ]\n",
      " [0.6746265  0.680948  ]\n",
      " [0.6761301  0.68200237]\n",
      " [0.6746451  0.68061733]\n",
      " [0.67714566 0.6835814 ]\n",
      " [0.6751761  0.6814831 ]\n",
      " [0.67606205 0.68238956]\n",
      " [0.6760505  0.68206966]\n",
      " [0.6760649  0.68228716]\n",
      " [0.6758051  0.68165493]\n",
      " [0.67583394 0.68213856]\n",
      " [0.67611426 0.6824621 ]\n",
      " [0.67461485 0.6806752 ]\n",
      " [0.6756427  0.6821913 ]\n",
      " [0.67603785 0.6823786 ]\n",
      " [0.6742057  0.67977935]\n",
      " [0.6760973  0.68265665]\n",
      " [0.67532986 0.6818648 ]\n",
      " [0.67646617 0.68263495]\n",
      " [0.67625123 0.6824845 ]\n",
      " [0.6766993  0.68279576]\n",
      " [0.6751403  0.68143314]\n",
      " [0.67438173 0.68037003]\n",
      " [0.67627686 0.68252456]\n",
      " [0.6767508  0.6832818 ]\n",
      " [0.6762805  0.68259925]\n",
      " [0.6748096  0.6809376 ]]\n",
      "sgd_pairs saved to sgd_pairs.pkl\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "sgd_pairs = np.array(sgd_pairs)\n",
    "print(sgd_pairs)\n",
    "\n",
    "# Save sgd_pairs to a pickle file\n",
    "with open('sgd_pairs.pkl', 'wb') as f:\n",
    "    pickle.dump(sgd_pairs, f)\n",
    "\n",
    "print(\"sgd_pairs saved to sgd_pairs.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cd1f6180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running SAGA constant\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 41/48 [1:21:26<12:42, 108.99s/it]"
     ]
    }
   ],
   "source": [
    "saga_pairs = []\n",
    "\n",
    "# SAGA constant\n",
    "print(\"Running SAGA constant\")\n",
    "key = init_prng(0)\n",
    "lr = LogisticRegression(X_train, Y_train, X_val, Y_val, beta = 0)\n",
    "w0 = lr.initialization()\n",
    "data_sampler = minibatch_sampler(N, 1)\n",
    "descent_func = lambda w, data_samples: lr.train_loss_and_grad(w, data_samples)[1]\n",
    "L = lr.lipschitz_constant() \n",
    "step_size = constant_step_size(0.1/L)\n",
    "\n",
    "# Initialize SAGA with gradients for all training data\n",
    "all_gradients = [lr.train_loss_and_grad(w0, [i])[1] for i in range(N)]\n",
    "saga_update = saga(all_gradients)\n",
    "update_method = saga_update\n",
    "wF, losses = optimize(w0, data_sampler, no_logger, descent_func, step_size, update_method, T, verbose=False)\n",
    "saga_pairs.append([lr.train_loss(wF), lr.validation_loss(wF)])\n",
    "\n",
    "# Release memory\n",
    "del lr, w0, all_gradients, saga_update, wF, losses\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "for i in tqdm(range(1,n_runs-1)): # change to n_runs-1 after testing\n",
    "    \n",
    "    # change random seed\n",
    "    key = init_prng(i)\n",
    "    lr = LogisticRegression(X_train, Y_train, X_val, Y_val, beta = 0)\n",
    "    w0 = lr.initialization()\n",
    "    # Initialize SAGA with gradients for all training data\n",
    "    all_gradients = [lr.train_loss_and_grad(w0, [i])[1] for i in range(N)]\n",
    "    saga_update = saga(all_gradients)\n",
    "    update_method = saga_update\n",
    "    wF, losses = optimize(w0, data_sampler, no_logger, descent_func, step_size, update_method, T, verbose=False)\n",
    "    saga_pairs.append([lr.train_loss(wF), lr.validation_loss(wF)])\n",
    "    \n",
    "    # Release memory after each iteration\n",
    "    del lr, w0, all_gradients, saga_update, wF, losses\n",
    "    gc.collect()\n",
    "\n",
    "saga_pairs = np.array(saga_pairs)\n",
    "print(saga_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c760041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save sgd_pairs to a pickle file\n",
    "with open('saga_pairs.pkl', 'wb') as f:\n",
    "    pickle.dump(saga_pairs, f)\n",
    "\n",
    "print(\"saga_pairs saved to sgd_pairs.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05b00bc",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7eabf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load arrays from pickle files \n",
    "with open('sgd_pairs.pkl', 'rb') as f:\n",
    "    sgd_pairs = pickle.load(f)\n",
    "\n",
    "with open('saga_pairs.pkl', 'rb') as f:\n",
    "    saga_pairs = pickle.load(f)\n",
    "\n",
    "scatterplot(sgd_pairs, saga_pairs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
